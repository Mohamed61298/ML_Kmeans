# -*- coding: utf-8 -*-
"""Untitled6.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15x-767KmJCbg4Tt4WmUHqCvDcUSv3srR
"""

# Commented out IPython magic to ensure Python compatibility.
# import Python libraries
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from mpl_toolkits.mplot3d import Axes3D

#create a list of columns for a dataset
column_names = ['CRIM', 'ZN','INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']
path = '/content/housing.csv'
#read the dataset
df = pd.read_csv(path, delim_whitespace=True, header = None, names = column_names)

train, validation, test = np.split(df.sample(frac=1), [int(0.6*len(df)),int(0.8*len(df))])

train

# Assign features and target variables to X and y
X = train.RM.values
y = train.MEDV.values

# Setup learning rate
l_rate = 0.00001

# Setup a number of iterations 
n_iters = 5000

# Assign a random integer to a slope and intercept
slope, intercept = np.random.rand(), np.random.rand()

# Store a number of feature values
length =len(X)

# Create starting point for calculation of partial derivatives
slope_deriv, intercept_deriv = 0, 0
    
# Create empty lists for storing the output of each iteration
cost_list, slope_list, intercept_list = [], [], []

# State the learning rate
learning_rate = l_rate

# State the number of iterations 
for i in range (n_iters):
    #formulate a linear equation
    y_pred = slope * X + intercept

    # cost function
    cost = 0
    for i in range(length):
        cost += ((slope * X[i] + intercept) - y[i]) ** 2
    cost = cost / length
    # Calculate partial derivatives of a cost function with respect to slope and to intercept
    for i in range(length):
        slope_deriv += ((slope * X[i] + intercept) - y[i]) * 2 * X[i]
        intercept_deriv += (slope * X[i] + intercept) - y[i]
    # Update slope and intercept
    slope -= 1/length * learning_rate * slope_deriv
    intercept -= 1/length * learning_rate * intercept_deriv
    # Appending lists for future processing
    slope_list.append(slope)
    intercept_list.append(intercept)
    cost_list.append(cost)

print(slope_list)
print(intercept_list)
print(slope)
print(intercept)

#------------------------------------------------------------------------------------------------
# Assign features and target variables to X and y
X = train.RM.values
y = train.MEDV.values
M=X**2
# Setup learning rate
l_rate = 0.00001

# Setup a number of iterations 
n_iters = 5000

# Assign a random integer to a slope and intercept
slope2 ,slope1, intercept =np.random.rand(), np.random.rand(), np.random.rand()

# Store a number of feature values
length =len(X)

# Create starting point for calculation of partial derivatives
slope2_deriv,slope1_deriv, intercept_deriv = 0,0, 0
    
# Create empty lists for storing the output of each iteration
cost_list,slope2_list, slope1_list, intercept_list = [], [], [],[]

# State the learning rate
learning_rate = l_rate

# State the number of iterations 
for i in range (n_iters):
    #formulate a linear equation
    y_pred = slope1 * X + intercept + slope2 *M

    # cost function
    cost = 0
    for i in range(length):
        cost += ((slope2*M[i]+slope1 * X[i] + intercept) - y[i]) ** 2
    cost = cost / length
    # Calculate partial derivatives of a cost function with respect to slope and to intercept
    for i in range(length):
        slope1_deriv += ((slope2*M[i]+slope1 * X[i] + intercept) - y[i]) * 2 * X[i]
        slope2_deriv += ((slope2*M[i]+slope1 * X[i] + intercept) - y[i]) * 4 * M[i]
        intercept_deriv += ((slope2*M[i]+slope1 * X[i] + intercept) - y[i])*2
    # Update slope and intercept
    slope1 -= 1/length * learning_rate * slope1_deriv
    slope2 -= 1/length * learning_rate * slope2_deriv
    intercept -= 1/length * learning_rate * intercept_deriv
    # Appending lists for future processing
    slope1_list.append(slope1)
    slope2_list.append(slope2)
    intercept_list.append(intercept)
    cost_list.append(cost)

# Calculate min of the cost function
cost_min = cost_list.index(np.min(cost_list))
print('The minimum of the cost function is {:.2f}, which can be found at {} iteration'.format(np.min(cost_list), cost_min))

def cost_plot():
    plt.plot(cost_list, label = 'Cost function')
    plt.scatter(cost_list.index(np.min(cost_list)), np.min(cost_list),
            color = 'red', marker = 'x', s = 70, label = 'Minimum')
    plt.xlabel('Iterations')
    plt.legend()
    plt.ylim(0, np.max(cost_list) + 100)

# Make some additions to visualization of the cost function and the minimum
plt.subplots(figsize=(16, 4))
plt.subplot(1,2,1)
cost_plot()
plt.ylabel('Value of cost function')
plt.title('Cost function with its minimum')
plt.xlim(0, n_iters)
plt.subplot(1,2,2)
cost_plot()
plt.title('Cost function with its minimum zoomed in')
plt.xlim(cost_min -100, cost_min +100);

model = LinearRegression().fit(X.reshape(-1,1), y.reshape(-1, 1))
pred = model.predict(X.reshape(-1, 1))
def single_regr_plot(prediction):
    plt.scatter(X, y, color = '#20B648',
            marker = 'o', edgecolor = '#56798B', alpha = 0.7, label = 'Original Data')
    plt.plot(X, prediction, color = '#15029A', label = 'Prediction')
    plt.xlabel("Average Number of Rooms per Dwelling")
    plt.legend();

# Plot linear regression
plt.subplots(figsize=(20, 8))
plt.subplot(1,2,1)
single_regr_plot(slope_list[cost_min] * X + intercept_list[cost_min])
plt.ylabel("Median value of owner-occupied homes in $1000's")
plt.title('Manual model')
#plt.subplot(1,2,2)
#single_regr_plot(pred)
#plt.title('Sklearn LinearRegression');

from sklearn.preprocessing import PolynomialFeatures
poly = PolynomialFeatures(degree=2, include_bias=False)
poly_features = poly.fit_transform(X.reshape(-1, 1))
from sklearn.linear_model import LinearRegression
poly_reg_model = LinearRegression()
poly_reg_model.fit(poly_features, y)
y_predicted = poly_reg_model.predict(poly_features)

plt.figure(figsize=(10, 6))
plt.title("Polynomial regression degree 2", size=16)
plt.scatter(X, y)
plt.scatter(X,y_predicted,c="red")
plt.show()

from sklearn.preprocessing import PolynomialFeatures
poly = PolynomialFeatures(degree=5, include_bias=False)
poly_features = poly.fit_transform(X.reshape(-1, 1))
from sklearn.linear_model import LinearRegression
poly_reg_model = LinearRegression()
poly_reg_model.fit(poly_features, y)
y_predicted = poly_reg_model.predict(poly_features)
plt.figure(figsize=(10, 6))
plt.title("Polynomial regression degree 5", size=16)
plt.scatter(X, y)
plt.scatter(X,y_predicted,c="red")
plt.show()

from sklearn.preprocessing import PolynomialFeatures
poly = PolynomialFeatures(degree=10, include_bias=False)
poly_features = poly.fit_transform(X.reshape(-1, 1))
from sklearn.linear_model import LinearRegression
poly_reg_model = LinearRegression()
poly_reg_model.fit(poly_features, y)
y_predicted = poly_reg_model.predict(poly_features)
plt.figure(figsize=(10, 6))
plt.title("Polynomial regression degree 10", size=16)
plt.scatter(X, y)
plt.scatter(X,y_predicted,c="red")
plt.show()